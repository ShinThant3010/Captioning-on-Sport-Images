{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocabulary import Vocabulary\n",
    "from data_loader import SportDataset\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import sys\n",
    "import random\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# The functional module contains helper functions for defining neural network layers as simple functions\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 100          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = False    # if True, load existing vocab file\n",
    "embed_size = 300           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 500        # number of training epochs\n",
    "save_every = 100             # determines frequency of saving model weights\n",
    "print_every = 5          # determines window for printing average loss\n",
    "log_file = 'training_log_100500.txt'       # name of file with saved training loss and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations to be applied on images\n",
    "transform = transforms.Compose([transforms.Resize((100, 100)),\n",
    "                                transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'Training Images'\n",
    "caption_file = 'Overall_Training_Captions_csv.csv'\n",
    "vocab = Vocabulary(image_path, caption_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab.vocab_size\n",
    "caption_lengths = vocab.caption_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params = params, lr = 0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(caption_lengths) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images : 1135\n"
     ]
    }
   ],
   "source": [
    "captionInIdx_file = 'image_index.csv'\n",
    "dir = 'Training Images'\n",
    "img_files = os.listdir(dir)\n",
    "\n",
    "def train_path(p): return os.path.join(dir, p)\n",
    "img_files = list(map(train_path, img_files))\n",
    "print('Number of images :', len(img_files))\n",
    "\n",
    "random.shuffle(img_files)\n",
    "train_files = img_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Step [5/12], Loss: 4.0095, Perplexity: 55.119237\n",
      "Epoch [1/500], Step [10/12], Loss: 3.2357, Perplexity: 25.4236\n",
      "Epoch [2/500], Step [5/12], Loss: 2.4887, Perplexity: 12.04528\n",
      "Epoch [2/500], Step [10/12], Loss: 2.2805, Perplexity: 9.7811\n",
      "Epoch [3/500], Step [5/12], Loss: 2.1096, Perplexity: 8.244898\n",
      "Epoch [3/500], Step [10/12], Loss: 2.0790, Perplexity: 7.9966\n",
      "Epoch [4/500], Step [5/12], Loss: 1.9606, Perplexity: 7.10380\n",
      "Epoch [4/500], Step [10/12], Loss: 1.9501, Perplexity: 7.0295\n",
      "Epoch [5/500], Step [5/12], Loss: 1.8358, Perplexity: 6.27014\n",
      "Epoch [5/500], Step [10/12], Loss: 1.8321, Perplexity: 6.2473\n",
      "Epoch [6/500], Step [5/12], Loss: 1.7213, Perplexity: 5.59151\n",
      "Epoch [6/500], Step [10/12], Loss: 1.7285, Perplexity: 5.6319\n",
      "Epoch [7/500], Step [5/12], Loss: 1.6227, Perplexity: 5.06665\n",
      "Epoch [7/500], Step [10/12], Loss: 1.6370, Perplexity: 5.1395\n",
      "Epoch [8/500], Step [5/12], Loss: 1.5357, Perplexity: 4.64465\n",
      "Epoch [8/500], Step [10/12], Loss: 1.5543, Perplexity: 4.7320\n",
      "Epoch [9/500], Step [5/12], Loss: 1.4586, Perplexity: 4.29999\n",
      "Epoch [9/500], Step [10/12], Loss: 1.4786, Perplexity: 4.3869\n",
      "Epoch [10/500], Step [5/12], Loss: 1.3864, Perplexity: 4.0004\n",
      "Epoch [10/500], Step [10/12], Loss: 1.4057, Perplexity: 4.0785\n",
      "Epoch [11/500], Step [5/12], Loss: 1.3177, Perplexity: 3.73509\n",
      "Epoch [11/500], Step [10/12], Loss: 1.3349, Perplexity: 3.7998\n",
      "Epoch [12/500], Step [5/12], Loss: 1.2513, Perplexity: 3.49502\n",
      "Epoch [12/500], Step [10/12], Loss: 1.2662, Perplexity: 3.5474\n",
      "Epoch [13/500], Step [5/12], Loss: 1.1918, Perplexity: 3.29291\n",
      "Epoch [13/500], Step [10/12], Loss: 1.2103, Perplexity: 3.3544\n",
      "Epoch [14/500], Step [5/12], Loss: 1.1488, Perplexity: 3.15455\n",
      "Epoch [14/500], Step [10/12], Loss: 1.1509, Perplexity: 3.1612\n",
      "Epoch [15/500], Step [5/12], Loss: 1.0923, Perplexity: 2.98117\n",
      "Epoch [15/500], Step [10/12], Loss: 1.0895, Perplexity: 2.9729\n",
      "Epoch [16/500], Step [5/12], Loss: 1.0292, Perplexity: 2.79899\n",
      "Epoch [16/500], Step [10/12], Loss: 1.0293, Perplexity: 2.7992\n",
      "Epoch [17/500], Step [5/12], Loss: 0.9655, Perplexity: 2.62621\n",
      "Epoch [17/500], Step [10/12], Loss: 0.9725, Perplexity: 2.6445\n",
      "Epoch [18/500], Step [5/12], Loss: 0.9131, Perplexity: 2.49218\n",
      "Epoch [18/500], Step [10/12], Loss: 0.9105, Perplexity: 2.4857\n",
      "Epoch [19/500], Step [5/12], Loss: 0.8576, Perplexity: 2.35748\n",
      "Epoch [19/500], Step [10/12], Loss: 0.8539, Perplexity: 2.3488\n",
      "Epoch [20/500], Step [5/12], Loss: 0.8131, Perplexity: 2.25506\n",
      "Epoch [20/500], Step [10/12], Loss: 0.8108, Perplexity: 2.2498\n",
      "Epoch [21/500], Step [5/12], Loss: 0.7807, Perplexity: 2.18290\n",
      "Epoch [21/500], Step [10/12], Loss: 0.7720, Perplexity: 2.1641\n",
      "Epoch [22/500], Step [5/12], Loss: 0.7365, Perplexity: 2.08870\n",
      "Epoch [22/500], Step [10/12], Loss: 0.7303, Perplexity: 2.0758\n",
      "Epoch [23/500], Step [5/12], Loss: 0.7049, Perplexity: 2.02379\n",
      "Epoch [23/500], Step [10/12], Loss: 0.6988, Perplexity: 2.0114\n",
      "Epoch [24/500], Step [5/12], Loss: 0.6637, Perplexity: 1.94204\n",
      "Epoch [24/500], Step [10/12], Loss: 0.6367, Perplexity: 1.8903\n",
      "Epoch [25/500], Step [5/12], Loss: 0.6109, Perplexity: 1.84210\n",
      "Epoch [25/500], Step [10/12], Loss: 0.6011, Perplexity: 1.8241\n",
      "Epoch [26/500], Step [5/12], Loss: 0.5737, Perplexity: 1.77492\n",
      "Epoch [26/500], Step [10/12], Loss: 0.5644, Perplexity: 1.7584\n",
      "Epoch [27/500], Step [5/12], Loss: 0.5430, Perplexity: 1.72122\n",
      "Epoch [27/500], Step [10/12], Loss: 0.5329, Perplexity: 1.7039\n",
      "Epoch [28/500], Step [5/12], Loss: 0.5177, Perplexity: 1.67822\n",
      "Epoch [28/500], Step [10/12], Loss: 0.5024, Perplexity: 1.6527\n",
      "Epoch [29/500], Step [5/12], Loss: 0.4879, Perplexity: 1.62909\n",
      "Epoch [29/500], Step [10/12], Loss: 0.4739, Perplexity: 1.6062\n",
      "Epoch [30/500], Step [5/12], Loss: 0.4590, Perplexity: 1.58250\n",
      "Epoch [30/500], Step [10/12], Loss: 0.4474, Perplexity: 1.5643\n",
      "Epoch [31/500], Step [5/12], Loss: 0.4362, Perplexity: 1.54696\n",
      "Epoch [31/500], Step [10/12], Loss: 0.4244, Perplexity: 1.5287\n",
      "Epoch [32/500], Step [5/12], Loss: 0.4092, Perplexity: 1.50565\n",
      "Epoch [32/500], Step [10/12], Loss: 0.4032, Perplexity: 1.4967\n",
      "Epoch [33/500], Step [5/12], Loss: 0.3882, Perplexity: 1.47437\n",
      "Epoch [33/500], Step [10/12], Loss: 0.3877, Perplexity: 1.4736\n",
      "Epoch [34/500], Step [5/12], Loss: 0.3729, Perplexity: 1.45205\n",
      "Epoch [34/500], Step [10/12], Loss: 0.3687, Perplexity: 1.4459\n",
      "Epoch [35/500], Step [5/12], Loss: 0.3554, Perplexity: 1.42670\n",
      "Epoch [35/500], Step [10/12], Loss: 0.3490, Perplexity: 1.4176\n",
      "Epoch [36/500], Step [5/12], Loss: 0.3434, Perplexity: 1.40983\n",
      "Epoch [36/500], Step [10/12], Loss: 0.3367, Perplexity: 1.4003\n",
      "Epoch [37/500], Step [5/12], Loss: 0.3310, Perplexity: 1.39241\n",
      "Epoch [37/500], Step [10/12], Loss: 0.3283, Perplexity: 1.3886\n",
      "Epoch [38/500], Step [5/12], Loss: 0.3170, Perplexity: 1.37302\n",
      "Epoch [38/500], Step [10/12], Loss: 0.3070, Perplexity: 1.3594\n",
      "Epoch [39/500], Step [5/12], Loss: 0.3023, Perplexity: 1.35299\n",
      "Epoch [39/500], Step [10/12], Loss: 0.2980, Perplexity: 1.3471\n",
      "Epoch [40/500], Step [5/12], Loss: 0.2898, Perplexity: 1.33627\n",
      "Epoch [40/500], Step [10/12], Loss: 0.2862, Perplexity: 1.3314\n",
      "Epoch [41/500], Step [5/12], Loss: 0.2819, Perplexity: 1.32570\n",
      "Epoch [41/500], Step [10/12], Loss: 0.2784, Perplexity: 1.3210\n",
      "Epoch [42/500], Step [5/12], Loss: 0.2735, Perplexity: 1.31460\n",
      "Epoch [42/500], Step [10/12], Loss: 0.2805, Perplexity: 1.3238\n",
      "Epoch [43/500], Step [5/12], Loss: 0.2701, Perplexity: 1.31023\n",
      "Epoch [43/500], Step [10/12], Loss: 0.2690, Perplexity: 1.3086\n",
      "Epoch [44/500], Step [5/12], Loss: 0.2604, Perplexity: 1.29752\n",
      "Epoch [44/500], Step [10/12], Loss: 0.2642, Perplexity: 1.3024\n",
      "Epoch [45/500], Step [5/12], Loss: 0.2519, Perplexity: 1.28656\n",
      "Epoch [45/500], Step [10/12], Loss: 0.2585, Perplexity: 1.2950\n",
      "Epoch [46/500], Step [5/12], Loss: 0.2456, Perplexity: 1.27840\n",
      "Epoch [46/500], Step [10/12], Loss: 0.2459, Perplexity: 1.2787\n",
      "Epoch [47/500], Step [5/12], Loss: 0.2381, Perplexity: 1.26880\n",
      "Epoch [47/500], Step [10/12], Loss: 0.2385, Perplexity: 1.2694\n",
      "Epoch [48/500], Step [5/12], Loss: 0.2267, Perplexity: 1.25457\n",
      "Epoch [48/500], Step [10/12], Loss: 0.2352, Perplexity: 1.2651\n",
      "Epoch [49/500], Step [5/12], Loss: 0.2292, Perplexity: 1.25763\n",
      "Epoch [49/500], Step [10/12], Loss: 0.2268, Perplexity: 1.2546\n",
      "Epoch [50/500], Step [5/12], Loss: 0.2165, Perplexity: 1.24180\n",
      "Epoch [50/500], Step [10/12], Loss: 0.2185, Perplexity: 1.2442\n",
      "Epoch [51/500], Step [5/12], Loss: 0.2106, Perplexity: 1.23452\n",
      "Epoch [51/500], Step [10/12], Loss: 0.2147, Perplexity: 1.2395\n",
      "Epoch [52/500], Step [5/12], Loss: 0.2062, Perplexity: 1.22905\n",
      "Epoch [52/500], Step [10/12], Loss: 0.2058, Perplexity: 1.2285\n",
      "Epoch [53/500], Step [5/12], Loss: 0.1969, Perplexity: 1.21779\n",
      "Epoch [53/500], Step [10/12], Loss: 0.2040, Perplexity: 1.2263\n",
      "Epoch [54/500], Step [5/12], Loss: 0.1941, Perplexity: 1.21420\n",
      "Epoch [54/500], Step [10/12], Loss: 0.2011, Perplexity: 1.2227\n",
      "Epoch [55/500], Step [5/12], Loss: 0.1926, Perplexity: 1.21246\n",
      "Epoch [55/500], Step [10/12], Loss: 0.1993, Perplexity: 1.2205\n",
      "Epoch [56/500], Step [5/12], Loss: 0.1924, Perplexity: 1.21211\n",
      "Epoch [56/500], Step [10/12], Loss: 0.1942, Perplexity: 1.2144\n",
      "Epoch [57/500], Step [5/12], Loss: 0.1906, Perplexity: 1.21004\n",
      "Epoch [57/500], Step [10/12], Loss: 0.1944, Perplexity: 1.2146\n",
      "Epoch [58/500], Step [5/12], Loss: 0.1932, Perplexity: 1.21329\n",
      "Epoch [58/500], Step [10/12], Loss: 0.1904, Perplexity: 1.2098\n",
      "Epoch [59/500], Step [5/12], Loss: 0.1831, Perplexity: 1.20092\n",
      "Epoch [59/500], Step [10/12], Loss: 0.1956, Perplexity: 1.2161\n",
      "Epoch [60/500], Step [5/12], Loss: 0.1756, Perplexity: 1.19196\n",
      "Epoch [60/500], Step [10/12], Loss: 0.1896, Perplexity: 1.2088\n",
      "Epoch [61/500], Step [5/12], Loss: 0.1790, Perplexity: 1.19613\n",
      "Epoch [61/500], Step [10/12], Loss: 0.1828, Perplexity: 1.2006\n",
      "Epoch [62/500], Step [5/12], Loss: 0.1833, Perplexity: 1.20123\n",
      "Epoch [62/500], Step [10/12], Loss: 0.1912, Perplexity: 1.2106\n",
      "Epoch [63/500], Step [5/12], Loss: 0.1878, Perplexity: 1.20660\n",
      "Epoch [63/500], Step [10/12], Loss: 0.1859, Perplexity: 1.2043\n",
      "Epoch [64/500], Step [5/12], Loss: 0.1835, Perplexity: 1.20143\n",
      "Epoch [64/500], Step [10/12], Loss: 0.1761, Perplexity: 1.1926\n",
      "Epoch [65/500], Step [5/12], Loss: 0.1725, Perplexity: 1.18839\n",
      "Epoch [65/500], Step [10/12], Loss: 0.1742, Perplexity: 1.1903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/500], Step [5/12], Loss: 0.1672, Perplexity: 1.18202\n",
      "Epoch [66/500], Step [10/12], Loss: 0.1666, Perplexity: 1.1813\n",
      "Epoch [67/500], Step [5/12], Loss: 0.1635, Perplexity: 1.17764\n",
      "Epoch [67/500], Step [10/12], Loss: 0.1691, Perplexity: 1.1842\n",
      "Epoch [68/500], Step [5/12], Loss: 0.1544, Perplexity: 1.16695\n",
      "Epoch [68/500], Step [10/12], Loss: 0.1552, Perplexity: 1.1679\n",
      "Epoch [69/500], Step [5/12], Loss: 0.1388, Perplexity: 1.14897\n",
      "Epoch [69/500], Step [10/12], Loss: 0.1450, Perplexity: 1.1561\n",
      "Epoch [70/500], Step [5/12], Loss: 0.1334, Perplexity: 1.14276\n",
      "Epoch [70/500], Step [10/12], Loss: 0.1343, Perplexity: 1.1437\n",
      "Epoch [71/500], Step [5/12], Loss: 0.1249, Perplexity: 1.13315\n",
      "Epoch [71/500], Step [10/12], Loss: 0.1347, Perplexity: 1.1442\n",
      "Epoch [72/500], Step [5/12], Loss: 0.1235, Perplexity: 1.13155\n",
      "Epoch [72/500], Step [10/12], Loss: 0.1373, Perplexity: 1.1471\n",
      "Epoch [73/500], Step [5/12], Loss: 0.1273, Perplexity: 1.13584\n",
      "Epoch [73/500], Step [10/12], Loss: 0.1348, Perplexity: 1.1444\n",
      "Epoch [74/500], Step [5/12], Loss: 0.1227, Perplexity: 1.13060\n",
      "Epoch [74/500], Step [10/12], Loss: 0.1251, Perplexity: 1.1333\n",
      "Epoch [75/500], Step [5/12], Loss: 0.1087, Perplexity: 1.11482\n",
      "Epoch [75/500], Step [10/12], Loss: 0.1182, Perplexity: 1.1254\n",
      "Epoch [76/500], Step [5/12], Loss: 0.1087, Perplexity: 1.11486\n",
      "Epoch [76/500], Step [10/12], Loss: 0.1119, Perplexity: 1.1184\n",
      "Epoch [77/500], Step [5/12], Loss: 0.0995, Perplexity: 1.10469\n",
      "Epoch [77/500], Step [10/12], Loss: 0.1076, Perplexity: 1.1136\n",
      "Epoch [78/500], Step [5/12], Loss: 0.1008, Perplexity: 1.10611\n",
      "Epoch [78/500], Step [10/12], Loss: 0.1123, Perplexity: 1.1189\n",
      "Epoch [79/500], Step [5/12], Loss: 0.1016, Perplexity: 1.10701\n",
      "Epoch [79/500], Step [10/12], Loss: 0.1045, Perplexity: 1.1102\n",
      "Epoch [80/500], Step [5/12], Loss: 0.1015, Perplexity: 1.10689\n",
      "Epoch [80/500], Step [10/12], Loss: 0.0968, Perplexity: 1.1016\n",
      "Epoch [81/500], Step [5/12], Loss: 0.0891, Perplexity: 1.09323\n",
      "Epoch [81/500], Step [10/12], Loss: 0.0943, Perplexity: 1.0989\n",
      "Epoch [82/500], Step [5/12], Loss: 0.0817, Perplexity: 1.08527\n",
      "Epoch [82/500], Step [10/12], Loss: 0.0844, Perplexity: 1.0880\n",
      "Epoch [83/500], Step [5/12], Loss: 0.0806, Perplexity: 1.08394\n",
      "Epoch [83/500], Step [10/12], Loss: 0.0797, Perplexity: 1.0830\n",
      "Epoch [84/500], Step [5/12], Loss: 0.0697, Perplexity: 1.07218\n",
      "Epoch [84/500], Step [10/12], Loss: 0.0756, Perplexity: 1.0786\n",
      "Epoch [85/500], Step [5/12], Loss: 0.0696, Perplexity: 1.07219\n",
      "Epoch [85/500], Step [10/12], Loss: 0.0800, Perplexity: 1.0833\n",
      "Epoch [86/500], Step [5/12], Loss: 0.0770, Perplexity: 1.08005\n",
      "Epoch [86/500], Step [10/12], Loss: 0.0885, Perplexity: 1.0925\n",
      "Epoch [87/500], Step [5/12], Loss: 0.0858, Perplexity: 1.08966\n",
      "Epoch [87/500], Step [10/12], Loss: 0.0788, Perplexity: 1.0820\n",
      "Epoch [88/500], Step [5/12], Loss: 0.0684, Perplexity: 1.07084\n",
      "Epoch [88/500], Step [10/12], Loss: 0.0626, Perplexity: 1.0646\n",
      "Epoch [89/500], Step [5/12], Loss: 0.0611, Perplexity: 1.06302\n",
      "Epoch [89/500], Step [10/12], Loss: 0.0559, Perplexity: 1.0575\n",
      "Epoch [90/500], Step [5/12], Loss: 0.0520, Perplexity: 1.05347\n",
      "Epoch [90/500], Step [10/12], Loss: 0.0486, Perplexity: 1.0497\n",
      "Epoch [91/500], Step [5/12], Loss: 0.0469, Perplexity: 1.04812\n",
      "Epoch [91/500], Step [10/12], Loss: 0.0486, Perplexity: 1.0498\n",
      "Epoch [92/500], Step [5/12], Loss: 0.0422, Perplexity: 1.04313\n",
      "Epoch [92/500], Step [10/12], Loss: 0.0485, Perplexity: 1.0497\n",
      "Epoch [93/500], Step [5/12], Loss: 0.0451, Perplexity: 1.04623\n",
      "Epoch [93/500], Step [10/12], Loss: 0.0392, Perplexity: 1.0399\n",
      "Epoch [94/500], Step [5/12], Loss: 0.0336, Perplexity: 1.03413\n",
      "Epoch [94/500], Step [10/12], Loss: 0.0335, Perplexity: 1.0341\n",
      "Epoch [95/500], Step [5/12], Loss: 0.0302, Perplexity: 1.03076\n",
      "Epoch [95/500], Step [10/12], Loss: 0.0272, Perplexity: 1.0276\n",
      "Epoch [96/500], Step [5/12], Loss: 0.0258, Perplexity: 1.02622\n",
      "Epoch [96/500], Step [10/12], Loss: 0.0252, Perplexity: 1.0255\n",
      "Epoch [97/500], Step [5/12], Loss: 0.0251, Perplexity: 1.02546\n",
      "Epoch [97/500], Step [10/12], Loss: 0.0241, Perplexity: 1.0244\n",
      "Epoch [98/500], Step [5/12], Loss: 0.0223, Perplexity: 1.02257\n",
      "Epoch [98/500], Step [10/12], Loss: 0.0243, Perplexity: 1.0246\n",
      "Epoch [99/500], Step [5/12], Loss: 0.0259, Perplexity: 1.02625\n",
      "Epoch [99/500], Step [10/12], Loss: 0.0339, Perplexity: 1.0344\n",
      "Epoch [100/500], Step [5/12], Loss: 0.0272, Perplexity: 1.0276\n",
      "Epoch [100/500], Step [10/12], Loss: 0.0249, Perplexity: 1.0252\n",
      "Epoch [101/500], Step [5/12], Loss: 0.0212, Perplexity: 1.02149\n",
      "Epoch [101/500], Step [10/12], Loss: 0.0227, Perplexity: 1.0229\n",
      "Epoch [102/500], Step [5/12], Loss: 0.0209, Perplexity: 1.02115\n",
      "Epoch [102/500], Step [10/12], Loss: 0.0198, Perplexity: 1.0200\n",
      "Epoch [103/500], Step [5/12], Loss: 0.0186, Perplexity: 1.01878\n",
      "Epoch [103/500], Step [10/12], Loss: 0.0162, Perplexity: 1.0163\n",
      "Epoch [104/500], Step [5/12], Loss: 0.0168, Perplexity: 1.01698\n",
      "Epoch [104/500], Step [10/12], Loss: 0.0146, Perplexity: 1.0147\n",
      "Epoch [105/500], Step [5/12], Loss: 0.0167, Perplexity: 1.01687\n",
      "Epoch [105/500], Step [10/12], Loss: 0.0175, Perplexity: 1.0177\n",
      "Epoch [106/500], Step [5/12], Loss: 0.0163, Perplexity: 1.01644\n",
      "Epoch [106/500], Step [10/12], Loss: 0.0161, Perplexity: 1.0162\n",
      "Epoch [107/500], Step [5/12], Loss: 0.0147, Perplexity: 1.01481\n",
      "Epoch [107/500], Step [10/12], Loss: 0.0244, Perplexity: 1.0247\n",
      "Epoch [108/500], Step [5/12], Loss: 0.0229, Perplexity: 1.02315\n",
      "Epoch [108/500], Step [10/12], Loss: 0.0280, Perplexity: 1.0284\n",
      "Epoch [109/500], Step [5/12], Loss: 0.0189, Perplexity: 1.01909\n",
      "Epoch [109/500], Step [10/12], Loss: 0.0142, Perplexity: 1.0143\n",
      "Epoch [110/500], Step [5/12], Loss: 0.0122, Perplexity: 1.01232\n",
      "Epoch [110/500], Step [10/12], Loss: 0.0131, Perplexity: 1.0131\n",
      "Epoch [111/500], Step [5/12], Loss: 0.0115, Perplexity: 1.01168\n",
      "Epoch [111/500], Step [10/12], Loss: 0.0117, Perplexity: 1.0118\n",
      "Epoch [112/500], Step [5/12], Loss: 0.0094, Perplexity: 1.00954\n",
      "Epoch [112/500], Step [10/12], Loss: 0.0083, Perplexity: 1.0083\n",
      "Epoch [113/500], Step [5/12], Loss: 0.0082, Perplexity: 1.00820\n",
      "Epoch [113/500], Step [10/12], Loss: 0.0077, Perplexity: 1.0077\n",
      "Epoch [114/500], Step [5/12], Loss: 0.0075, Perplexity: 1.00768\n",
      "Epoch [114/500], Step [10/12], Loss: 0.0072, Perplexity: 1.0072\n",
      "Epoch [115/500], Step [5/12], Loss: 0.0071, Perplexity: 1.00716\n",
      "Epoch [115/500], Step [10/12], Loss: 0.0069, Perplexity: 1.0069\n",
      "Epoch [116/500], Step [5/12], Loss: 0.0068, Perplexity: 1.00685\n",
      "Epoch [116/500], Step [10/12], Loss: 0.0066, Perplexity: 1.0066\n",
      "Epoch [117/500], Step [5/12], Loss: 0.0065, Perplexity: 1.00654\n",
      "Epoch [117/500], Step [10/12], Loss: 0.0064, Perplexity: 1.0064\n",
      "Epoch [118/500], Step [5/12], Loss: 0.0063, Perplexity: 1.00634\n",
      "Epoch [118/500], Step [10/12], Loss: 0.0061, Perplexity: 1.0062\n",
      "Epoch [119/500], Step [5/12], Loss: 0.0061, Perplexity: 1.00613\n",
      "Epoch [119/500], Step [10/12], Loss: 0.0059, Perplexity: 1.0060\n",
      "Epoch [120/500], Step [5/12], Loss: 0.0059, Perplexity: 1.00592\n",
      "Epoch [120/500], Step [10/12], Loss: 0.0058, Perplexity: 1.0058\n",
      "Epoch [121/500], Step [5/12], Loss: 0.0057, Perplexity: 1.00572\n",
      "Epoch [121/500], Step [10/12], Loss: 0.0056, Perplexity: 1.0056\n",
      "Epoch [122/500], Step [5/12], Loss: 0.0055, Perplexity: 1.00561\n",
      "Epoch [122/500], Step [10/12], Loss: 0.0054, Perplexity: 1.0054\n",
      "Epoch [123/500], Step [5/12], Loss: 0.0054, Perplexity: 1.00540\n",
      "Epoch [123/500], Step [10/12], Loss: 0.0053, Perplexity: 1.0053\n",
      "Epoch [124/500], Step [5/12], Loss: 0.0052, Perplexity: 1.00520\n",
      "Epoch [124/500], Step [10/12], Loss: 0.0051, Perplexity: 1.0051\n",
      "Epoch [125/500], Step [5/12], Loss: 0.0051, Perplexity: 1.00519\n",
      "Epoch [125/500], Step [10/12], Loss: 0.0050, Perplexity: 1.0050\n",
      "Epoch [126/500], Step [5/12], Loss: 0.0050, Perplexity: 1.00509\n",
      "Epoch [126/500], Step [10/12], Loss: 0.0049, Perplexity: 1.0049\n",
      "Epoch [127/500], Step [5/12], Loss: 0.0048, Perplexity: 1.00488\n",
      "Epoch [127/500], Step [10/12], Loss: 0.0047, Perplexity: 1.0048\n",
      "Epoch [128/500], Step [5/12], Loss: 0.0047, Perplexity: 1.00478\n",
      "Epoch [128/500], Step [10/12], Loss: 0.0046, Perplexity: 1.0046\n",
      "Epoch [129/500], Step [5/12], Loss: 0.0046, Perplexity: 1.00468\n",
      "Epoch [129/500], Step [10/12], Loss: 0.0045, Perplexity: 1.0045\n",
      "Epoch [130/500], Step [5/12], Loss: 0.0045, Perplexity: 1.00457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/500], Step [10/12], Loss: 0.0044, Perplexity: 1.0044\n",
      "Epoch [131/500], Step [5/12], Loss: 0.0044, Perplexity: 1.00447\n",
      "Epoch [131/500], Step [10/12], Loss: 0.0043, Perplexity: 1.0043\n",
      "Epoch [132/500], Step [5/12], Loss: 0.0043, Perplexity: 1.00436\n",
      "Epoch [132/500], Step [10/12], Loss: 0.0042, Perplexity: 1.0042\n",
      "Epoch [133/500], Step [5/12], Loss: 0.0042, Perplexity: 1.00426\n",
      "Epoch [133/500], Step [10/12], Loss: 0.0041, Perplexity: 1.0041\n",
      "Epoch [134/500], Step [5/12], Loss: 0.0041, Perplexity: 1.00416\n",
      "Epoch [134/500], Step [10/12], Loss: 0.0040, Perplexity: 1.0040\n",
      "Epoch [135/500], Step [5/12], Loss: 0.0040, Perplexity: 1.00405\n",
      "Epoch [135/500], Step [10/12], Loss: 0.0039, Perplexity: 1.0039\n",
      "Epoch [136/500], Step [5/12], Loss: 0.0039, Perplexity: 1.00395\n",
      "Epoch [136/500], Step [10/12], Loss: 0.0039, Perplexity: 1.0039\n",
      "Epoch [137/500], Step [5/12], Loss: 0.0038, Perplexity: 1.00385\n",
      "Epoch [137/500], Step [10/12], Loss: 0.0038, Perplexity: 1.0038\n",
      "Epoch [138/500], Step [5/12], Loss: 0.0038, Perplexity: 1.00385\n",
      "Epoch [138/500], Step [10/12], Loss: 0.0037, Perplexity: 1.0037\n",
      "Epoch [139/500], Step [5/12], Loss: 0.0037, Perplexity: 1.00374\n",
      "Epoch [139/500], Step [10/12], Loss: 0.0036, Perplexity: 1.0036\n",
      "Epoch [140/500], Step [5/12], Loss: 0.0036, Perplexity: 1.00364\n",
      "Epoch [140/500], Step [10/12], Loss: 0.0036, Perplexity: 1.0036\n",
      "Epoch [141/500], Step [5/12], Loss: 0.0035, Perplexity: 1.00354\n",
      "Epoch [141/500], Step [10/12], Loss: 0.0035, Perplexity: 1.0035\n",
      "Epoch [142/500], Step [5/12], Loss: 0.0035, Perplexity: 1.00354\n",
      "Epoch [142/500], Step [10/12], Loss: 0.0034, Perplexity: 1.0034\n",
      "Epoch [143/500], Step [5/12], Loss: 0.0034, Perplexity: 1.00343\n",
      "Epoch [143/500], Step [10/12], Loss: 0.0034, Perplexity: 1.0034\n",
      "Epoch [144/500], Step [5/12], Loss: 0.0033, Perplexity: 1.00333\n",
      "Epoch [144/500], Step [10/12], Loss: 0.0033, Perplexity: 1.0033\n",
      "Epoch [145/500], Step [5/12], Loss: 0.0033, Perplexity: 1.00333\n",
      "Epoch [145/500], Step [10/12], Loss: 0.0032, Perplexity: 1.0032\n",
      "Epoch [146/500], Step [5/12], Loss: 0.0032, Perplexity: 1.00323\n",
      "Epoch [146/500], Step [10/12], Loss: 0.0032, Perplexity: 1.0032\n",
      "Epoch [147/500], Step [5/12], Loss: 0.0032, Perplexity: 1.00322\n",
      "Epoch [147/500], Step [10/12], Loss: 0.0031, Perplexity: 1.0031\n",
      "Epoch [148/500], Step [5/12], Loss: 0.0031, Perplexity: 1.00312\n",
      "Epoch [148/500], Step [10/12], Loss: 0.0031, Perplexity: 1.0031\n",
      "Epoch [149/500], Step [5/12], Loss: 0.0030, Perplexity: 1.00302\n",
      "Epoch [149/500], Step [10/12], Loss: 0.0030, Perplexity: 1.0030\n",
      "Epoch [150/500], Step [5/12], Loss: 0.0030, Perplexity: 1.00302\n",
      "Epoch [150/500], Step [10/12], Loss: 0.0029, Perplexity: 1.0030\n",
      "Epoch [151/500], Step [5/12], Loss: 0.0029, Perplexity: 1.00292\n",
      "Epoch [151/500], Step [10/12], Loss: 0.0029, Perplexity: 1.0029\n",
      "Epoch [152/500], Step [5/12], Loss: 0.0029, Perplexity: 1.00291\n",
      "Epoch [152/500], Step [10/12], Loss: 0.0028, Perplexity: 1.0029\n",
      "Epoch [153/500], Step [5/12], Loss: 0.0028, Perplexity: 1.00281\n",
      "Epoch [153/500], Step [10/12], Loss: 0.0028, Perplexity: 1.0028\n",
      "Epoch [154/500], Step [5/12], Loss: 0.0028, Perplexity: 1.00281\n",
      "Epoch [154/500], Step [10/12], Loss: 0.0028, Perplexity: 1.0028\n",
      "Epoch [155/500], Step [5/12], Loss: 0.0027, Perplexity: 1.00271\n",
      "Epoch [155/500], Step [10/12], Loss: 0.0027, Perplexity: 1.0027\n",
      "Epoch [156/500], Step [5/12], Loss: 0.0027, Perplexity: 1.00271\n",
      "Epoch [156/500], Step [10/12], Loss: 0.0027, Perplexity: 1.0027\n",
      "Epoch [157/500], Step [5/12], Loss: 0.0027, Perplexity: 1.00270\n",
      "Epoch [157/500], Step [10/12], Loss: 0.0026, Perplexity: 1.0026\n",
      "Epoch [158/500], Step [5/12], Loss: 0.0026, Perplexity: 1.00260\n",
      "Epoch [158/500], Step [10/12], Loss: 0.0026, Perplexity: 1.0026\n",
      "Epoch [159/500], Step [5/12], Loss: 0.0026, Perplexity: 1.00260\n",
      "Epoch [159/500], Step [10/12], Loss: 0.0025, Perplexity: 1.0025\n",
      "Epoch [160/500], Step [5/12], Loss: 0.0025, Perplexity: 1.00250\n",
      "Epoch [160/500], Step [10/12], Loss: 0.0025, Perplexity: 1.0025\n",
      "Epoch [161/500], Step [5/12], Loss: 0.0025, Perplexity: 1.00250\n",
      "Epoch [161/500], Step [10/12], Loss: 0.0025, Perplexity: 1.0025\n",
      "Epoch [162/500], Step [5/12], Loss: 0.0024, Perplexity: 1.00240\n",
      "Epoch [162/500], Step [10/12], Loss: 0.0024, Perplexity: 1.0024\n",
      "Epoch [163/500], Step [5/12], Loss: 0.0024, Perplexity: 1.00240\n",
      "Epoch [163/500], Step [10/12], Loss: 0.0024, Perplexity: 1.0024\n",
      "Epoch [164/500], Step [5/12], Loss: 0.0024, Perplexity: 1.00249\n",
      "Epoch [164/500], Step [10/12], Loss: 0.0023, Perplexity: 1.0023\n",
      "Epoch [165/500], Step [5/12], Loss: 0.0023, Perplexity: 1.00239\n",
      "Epoch [165/500], Step [10/12], Loss: 0.0023, Perplexity: 1.0023\n",
      "Epoch [166/500], Step [5/12], Loss: 0.0023, Perplexity: 1.00239\n",
      "Epoch [166/500], Step [10/12], Loss: 0.0023, Perplexity: 1.0023\n",
      "Epoch [167/500], Step [5/12], Loss: 0.0023, Perplexity: 1.00239\n",
      "Epoch [167/500], Step [10/12], Loss: 0.0022, Perplexity: 1.0022\n",
      "Epoch [168/500], Step [5/12], Loss: 0.0022, Perplexity: 1.00229\n",
      "Epoch [168/500], Step [10/12], Loss: 0.0022, Perplexity: 1.0022\n",
      "Epoch [169/500], Step [5/12], Loss: 0.0022, Perplexity: 1.00229\n",
      "Epoch [169/500], Step [10/12], Loss: 0.0022, Perplexity: 1.0022\n",
      "Epoch [170/500], Step [5/12], Loss: 0.0022, Perplexity: 1.00229\n",
      "Epoch [170/500], Step [10/12], Loss: 0.0021, Perplexity: 1.0021\n",
      "Epoch [171/500], Step [5/12], Loss: 0.0021, Perplexity: 1.00219\n",
      "Epoch [171/500], Step [10/12], Loss: 0.0021, Perplexity: 1.0021\n",
      "Epoch [172/500], Step [5/12], Loss: 0.0021, Perplexity: 1.00218\n",
      "Epoch [172/500], Step [10/12], Loss: 0.0021, Perplexity: 1.0021\n",
      "Epoch [173/500], Step [5/12], Loss: 0.0021, Perplexity: 1.00218\n",
      "Epoch [173/500], Step [10/12], Loss: 0.0021, Perplexity: 1.0021\n",
      "Epoch [174/500], Step [5/12], Loss: 0.0020, Perplexity: 1.00208\n",
      "Epoch [174/500], Step [10/12], Loss: 0.0020, Perplexity: 1.0020\n",
      "Epoch [175/500], Step [5/12], Loss: 0.0020, Perplexity: 1.00208\n",
      "Epoch [175/500], Step [10/12], Loss: 0.0020, Perplexity: 1.0020\n",
      "Epoch [176/500], Step [5/12], Loss: 0.0020, Perplexity: 1.00208\n",
      "Epoch [176/500], Step [10/12], Loss: 0.0020, Perplexity: 1.0020\n",
      "Epoch [177/500], Step [5/12], Loss: 0.0020, Perplexity: 1.00208\n",
      "Epoch [177/500], Step [10/12], Loss: 0.0019, Perplexity: 1.0019\n",
      "Epoch [178/500], Step [5/12], Loss: 0.0019, Perplexity: 1.00198\n",
      "Epoch [178/500], Step [10/12], Loss: 0.0019, Perplexity: 1.0019\n",
      "Epoch [179/500], Step [5/12], Loss: 0.0019, Perplexity: 1.00198\n",
      "Epoch [179/500], Step [10/12], Loss: 0.0019, Perplexity: 1.0019\n",
      "Epoch [180/500], Step [5/12], Loss: 0.0019, Perplexity: 1.00198\n",
      "Epoch [180/500], Step [10/12], Loss: 0.0019, Perplexity: 1.0019\n",
      "Epoch [181/500], Step [5/12], Loss: 0.0019, Perplexity: 1.00197\n",
      "Epoch [181/500], Step [10/12], Loss: 0.0018, Perplexity: 1.0018\n",
      "Epoch [182/500], Step [5/12], Loss: 0.0018, Perplexity: 1.00187\n",
      "Epoch [182/500], Step [10/12], Loss: 0.0018, Perplexity: 1.0018\n",
      "Epoch [183/500], Step [5/12], Loss: 0.0018, Perplexity: 1.00187\n",
      "Epoch [183/500], Step [10/12], Loss: 0.0018, Perplexity: 1.0018\n",
      "Epoch [184/500], Step [5/12], Loss: 0.0018, Perplexity: 1.00187\n",
      "Epoch [184/500], Step [10/12], Loss: 0.0018, Perplexity: 1.0018\n",
      "Epoch [185/500], Step [5/12], Loss: 0.0018, Perplexity: 1.00187\n",
      "Epoch [185/500], Step [10/12], Loss: 0.0017, Perplexity: 1.0017\n",
      "Epoch [186/500], Step [5/12], Loss: 0.0017, Perplexity: 1.00177\n",
      "Epoch [186/500], Step [10/12], Loss: 0.0017, Perplexity: 1.0017\n",
      "Epoch [187/500], Step [5/12], Loss: 0.0017, Perplexity: 1.00177\n",
      "Epoch [187/500], Step [10/12], Loss: 0.0017, Perplexity: 1.0017\n",
      "Epoch [188/500], Step [5/12], Loss: 0.0017, Perplexity: 1.00177\n",
      "Epoch [188/500], Step [10/12], Loss: 0.0017, Perplexity: 1.0017\n",
      "Epoch [189/500], Step [5/12], Loss: 0.0017, Perplexity: 1.00177\n",
      "Epoch [189/500], Step [10/12], Loss: 0.0016, Perplexity: 1.0017\n",
      "Epoch [190/500], Step [5/12], Loss: 0.0016, Perplexity: 1.00167\n",
      "Epoch [190/500], Step [10/12], Loss: 0.0016, Perplexity: 1.0016\n",
      "Epoch [191/500], Step [5/12], Loss: 0.0016, Perplexity: 1.00167\n",
      "Epoch [191/500], Step [10/12], Loss: 0.0016, Perplexity: 1.0016\n",
      "Epoch [192/500], Step [5/12], Loss: 0.0016, Perplexity: 1.00166\n",
      "Epoch [192/500], Step [10/12], Loss: 0.0016, Perplexity: 1.0016\n",
      "Epoch [193/500], Step [5/12], Loss: 0.0016, Perplexity: 1.00166\n",
      "Epoch [193/500], Step [10/12], Loss: 0.0016, Perplexity: 1.0016\n",
      "Epoch [194/500], Step [5/12], Loss: 0.0016, Perplexity: 1.00166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [194/500], Step [10/12], Loss: 0.0015, Perplexity: 1.0015\n",
      "Epoch [195/500], Step [5/12], Loss: 0.0015, Perplexity: 1.00156\n",
      "Epoch [195/500], Step [10/12], Loss: 0.0015, Perplexity: 1.0015\n",
      "Epoch [196/500], Step [5/12], Loss: 0.0015, Perplexity: 1.00156\n",
      "Epoch [196/500], Step [10/12], Loss: 0.0015, Perplexity: 1.0015\n",
      "Epoch [197/500], Step [5/12], Loss: 0.0015, Perplexity: 1.00156\n",
      "Epoch [197/500], Step [10/12], Loss: 0.0015, Perplexity: 1.0015\n",
      "Epoch [198/500], Step [5/12], Loss: 0.0015, Perplexity: 1.00156\n",
      "Epoch [198/500], Step [10/12], Loss: 0.0015, Perplexity: 1.0015\n",
      "Epoch [199/500], Step [5/12], Loss: 0.0015, Perplexity: 1.00156\n",
      "Epoch [199/500], Step [10/12], Loss: 0.0015, Perplexity: 1.0015\n",
      "Epoch [200/500], Step [5/12], Loss: 0.0014, Perplexity: 1.00156\n",
      "Epoch [200/500], Step [10/12], Loss: 0.0014, Perplexity: 1.0014\n",
      "Epoch [201/500], Step [5/12], Loss: 0.0014, Perplexity: 1.00146\n",
      "Epoch [201/500], Step [10/12], Loss: 0.0014, Perplexity: 1.0014\n",
      "Epoch [202/500], Step [5/12], Loss: 0.0014, Perplexity: 1.00146\n",
      "Epoch [202/500], Step [10/12], Loss: 0.0014, Perplexity: 1.0014\n",
      "Epoch [203/500], Step [5/12], Loss: 0.0014, Perplexity: 1.00146\n",
      "Epoch [203/500], Step [10/12], Loss: 0.0014, Perplexity: 1.0014\n",
      "Epoch [204/500], Step [5/12], Loss: 0.0014, Perplexity: 1.00146\n",
      "Epoch [204/500], Step [10/12], Loss: 0.0014, Perplexity: 1.0014\n",
      "Epoch [205/500], Step [5/12], Loss: 0.0014, Perplexity: 1.00146\n",
      "Epoch [205/500], Step [10/12], Loss: 0.0014, Perplexity: 1.0014\n",
      "Epoch [206/500], Step [5/12], Loss: 0.0013, Perplexity: 1.00135\n",
      "Epoch [206/500], Step [8/12], Loss: 0.0013, Perplexity: 1.0013"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8a5cc205ae5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi_step\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_step\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;31m#print(images.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#print(captions.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\PP_ML_Project_AdjustModel\\data_loader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mcopy_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mANTIALIAS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0moriginal_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPLI_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mimageName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m#print(imageName)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \"\"\"\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    train_ds = SportDataset(train_files, captionInIdx_file, transform, mode = 'train')\n",
    "    train_dl = DataLoader(train_ds, batch_size = batch_size)\n",
    "    #print(len(train_ds), len(train_dl))\n",
    "    \n",
    "    # shape of training data\n",
    "    dataiter = iter(train_dl)\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "\n",
    "        images, captions = dataiter.next()\n",
    "        #print(images.shape)\n",
    "        #print(captions.shape)\n",
    "        \n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models_100500', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models_100500', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
